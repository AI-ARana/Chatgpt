import torch
import torch.nn as nn
from torch.nn import TransformerEncoder, TransformerEncoderLayer

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embedding_size, num_heads, hidden_size, num_layers):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        encoder_layers = TransformerEncoderLayer(embedding_size, num_heads, hidden_size)
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)
        self.fc = nn.Linear(embedding_size, vocab_size)

    def forward(self, src):
        embedded = self.embedding(src)
        transformer_output = self.transformer_encoder(embedded)
        output = self.fc(transformer_output[-1])  # Only take the output from the last layer
        return output

# Example usage
model = TransformerModel(vocab_size=10000, embedding_size=128, num_heads=4, hidden_size=256, num_layers=2)
input_sequence = torch.tensor([[1, 2, 3, 4, 5]])  # Example input sequence
output = model(input_sequence)
print(output.shape)  # Example debugging output
